{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/software/Anaconda3-5.0.1-el7-x86_64/envs/DL_GPU_cuda_9.0/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import collections\n",
    "import pickle\n",
    "import argparse\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--data_dir', default='./')\n",
    "parser.add_argument('--save_dir', default='./')\n",
    "# Dimension of hidden layer variables h and c\n",
    "parser.add_argument('--num_units', default=128*2)\n",
    "parser.add_argument('--batch_size', default=64)\n",
    "# Number of steps in each batch for training\n",
    "parser.add_argument('--num_steps', default=75)\n",
    "parser.add_argument('--num_epochs', default=30)\n",
    "# Time step\n",
    "parser.add_argument('--lr', default=0.002)\n",
    "# Number of possible inputs/outputs \n",
    "parser.add_argument('--num_chars')\n",
    "parser.add_argument('--num_batches',default=20)\n",
    "args, unparsed = parser.parse_known_args()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define a timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timer(start, end):\n",
    "    hrs, rem = divmod(end-start, 3600)\n",
    "    mins, secs = divmod(rem, 60)\n",
    "    print('{:0>2} hours {:0>2} minutes {:05.2f} seconds'.format(int(hrs), int(mins), secs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load text, vreate vocabulary, define batches from text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextLoader():\n",
    "\n",
    "    def __init__(self, data_dir, batch_size=64, seq_length=50, encoding='utf-8'):\n",
    "        self.data_dir = data_dir\n",
    "        self.encoding = encoding\n",
    "        self.batch_size = batch_size\n",
    "        self.seq_length = seq_length\n",
    "\n",
    "        self.input_file = os.path.join(data_dir, 'tinyshakespeare.txt')\n",
    "        self.vocab_file = os.path.join(data_dir, 'vocab.pkl')\n",
    "        # Numeric file of characters translated to indices.\n",
    "        self.tensor_file = os.path.join(data_dir, 'data.npy')\n",
    "        \n",
    "        if not (os.path.exists(self.vocab_file) and os.path.exists(self.tensor_file)):\n",
    "            print('it seems we havent processed the text data yet: reading the shakespear')\n",
    "            self.preprocess(self.input_file, self.vocab_file, self.tensor_file)\n",
    "        else:\n",
    "            print('there are preprocessed data - lets load it')\n",
    "            self.load_preprocessed(self.vocab_file, self.tensor_file)\n",
    "\n",
    "        self.create_batches()\n",
    "        self.reset_batch_pointer()\n",
    "\n",
    "    # Create numeric file.\n",
    "    def preprocess(self, input_file=None, vocab_file=None, tensor_file=None, saveit=True):\n",
    "        if input_file is not None:\n",
    "            self.input_file = input_file\n",
    "        if vocab_file is not None:\n",
    "            self.vocab_file = vocab_file\n",
    "        if tensor_file is not None:\n",
    "            self.tensor_file = tensor_file\n",
    "\n",
    "        with open(self.input_file, 'r') as f:\n",
    "            data = f.read()\n",
    "        #data = data.lower()\n",
    "        self.total_length = len(data)\n",
    "        counter = collections.Counter(data)\n",
    "        count_pairs = sorted(counter.items(), key=lambda x: -x[1])\n",
    "        self.chars, _ = zip(*count_pairs)\n",
    "        self.vocab_size = len(self.chars)\n",
    "        self.vocab_to_idx = dict(zip(self.chars, range(len(self.chars))))\n",
    "        self.idx_to_vocab = dict(zip(self.vocab_to_idx.values(), self.vocab_to_idx.keys()))\n",
    "\n",
    "        if saveit:\n",
    "            with open(self.vocab_file, 'wb') as f:  # saving dictionary so we don't compute it again\n",
    "                pickle.dump(self.chars, f)\n",
    "            self.tensor = np.array(list(map(self.vocab_to_idx.get, data)))\n",
    "            np.save(self.tensor_file, self.tensor)  # saving the numerified data\n",
    "    # Load numeric file create dictionaries for char2idx and back\n",
    "    def load_preprocessed(self, vocab_file=None, tensor_file=None):\n",
    "        if vocab_file is not None:\n",
    "            self.vocab_file = vocab_file\n",
    "        if tensor_file is not None:\n",
    "            self.tensor_file = tensor_file\n",
    "\n",
    "        with open(self.vocab_file, 'rb') as f:\n",
    "            self.chars = pickle.load(f)\n",
    "\n",
    "        # attributes\n",
    "        self.vocab_size = len(self.chars)\n",
    "        self.vocab = dict(zip(self.chars, range(self.vocab_size)))\n",
    "        self.vocab_to_idx = dict(zip(self.chars, range(len(self.chars))))\n",
    "        self.idx_to_vocab = dict(zip(self.vocab_to_idx.values(), self.vocab_to_idx.keys()))\n",
    "        self.tensor = np.load(tensor_file)\n",
    "        self.num_batches = int(self.tensor.size / (self.batch_size * self.seq_length))\n",
    "            \n",
    "    # tensor size = the length of the entire data sequence\n",
    "    # divide into batch_size sub sequences and stack\n",
    "    # cut those by seq_length to produce batches of [batch size, seq_length] sized examples\n",
    "    def create_batches(self):\n",
    "\n",
    "        \n",
    "        self.num_batches = int(self.tensor.size / (self.batch_size * self.seq_length))\n",
    "\n",
    "        if self.num_batches == 0:\n",
    "            assert False, 'Not enough data. Make seq_length and/or batch_size smaller'\n",
    "\n",
    "        self.tensor = self.tensor[:self.num_batches * self.batch_size * self.seq_length]  # so we get an even divide\n",
    "        xdata = self.tensor\n",
    "        ydata = np.copy(self.tensor)\n",
    "\n",
    "        # ydata is one step ahead of x and last item is first item of x \n",
    "        # to get sequences of same length    \n",
    "        ydata[:-1] = xdata[1:] \n",
    "        ydata[-1] = xdata[0]\n",
    "\n",
    "        self.x_batches = np.split(xdata.reshape(self.batch_size, -1), self.num_batches, 1)\n",
    "        self.y_batches = np.split(ydata.reshape(self.batch_size, -1), self.num_batches, 1)\n",
    "        \n",
    "        self.train_num_batches=np.int32(self.num_batches*.8)\n",
    "        self.test_num_batches=self.num_batches-self.train_num_batches\n",
    "        self.train_x_batches=self.x_batches[0:self.train_num_batches]\n",
    "        self.train_y_batches=self.y_batches[0:self.train_num_batches]\n",
    "        self.test_x_batches=self.x_batches[self.train_num_batches:]\n",
    "        self.test_y_batches=self.y_batches[self.train_num_batches:]\n",
    "\n",
    "        # xdata: L length\n",
    "        # xdata reshaped: batch_size, (L/batch_size) length following natural indexing\n",
    "        # np.split: into num batches batches along the width(sentence)\n",
    "\n",
    "    def next_batch_train(self):\n",
    "        x, y = self.train_x_batches[self.pointer], self.train_y_batches[self.pointer]\n",
    "        self.pointer += 1\n",
    "        return x,y\n",
    "    \n",
    "    def next_batch_test(self):\n",
    "        x, y = self.test_x_batches[self.pointer], self.test_y_batches[self.pointer]\n",
    "        self.pointer += 1\n",
    "        return x,y\n",
    "\n",
    "    def reset_batch_pointer(self):\n",
    "        self.pointer = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are preprocessed data - lets load it\n",
      "num chars 65\n",
      "num batches 232\n"
     ]
    }
   ],
   "source": [
    "loader = TextLoader(args.data_dir, batch_size=args.batch_size, seq_length=args.num_steps)\n",
    "args.num_chars = loader.vocab_size\n",
    "print('num chars',args.num_chars)\n",
    "print('num batches',loader.num_batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic RNN cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyBasicRNNCell(tf.contrib.rnn.BasicRNNCell):\n",
    "\n",
    "    def build(self, inputs_shape):\n",
    "\n",
    "        input_depth = inputs_shape[1].value\n",
    "        \n",
    "        self._kernel = self.add_variable(name=\"kernel_hidden\", shape=[input_depth + self._num_units, self._num_units])\n",
    "        self._bias = self.add_variable(name=\"bias_hidden\", shape=[self._num_units], initializer=tf.zeros_initializer())\n",
    "        \n",
    "        self.built = True\n",
    "\n",
    "    def call(self, inputs, state):\n",
    "        \"\"\"Most basic RNN: output = new_state = act(W * input + U * state + B).\"\"\"\n",
    "        \n",
    "        output = tf.tanh(tf.matmul(tf.concat([inputs, state], 1), self._kernel) + self._bias)\n",
    "\n",
    "        return output, output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN cell with one intermediate layer before output hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyBasicRNNCell_d2(tf.contrib.rnn.BasicRNNCell):\n",
    "\n",
    "    def build(self, inputs_shape):\n",
    "\n",
    "        input_depth = inputs_shape[1].value\n",
    "        \n",
    "        self._kernel = self.add_variable(name=\"kernel_hidden\", shape=[input_depth + self._num_units, self._num_units])\n",
    "        self._bias = self.add_variable(name=\"bias_hidden\", shape=[self._num_units], initializer=tf.zeros_initializer())\n",
    "        self._kernel1 = self.add_variable(name=\"kernel_hidden1\", shape=[self._num_units, self._num_units])\n",
    "        self._bias1 = self.add_variable(name=\"bias_hidden1\", shape=[self._num_units], initializer=tf.zeros_initializer())\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, inputs, state):\n",
    "        \"\"\"Most basic RNN: output = new_state = act(W * input + U * state + B).\"\"\"\n",
    "        \n",
    "        inter1 = tf.tanh(tf.matmul(tf.concat([inputs, state], 1), self._kernel) + self._bias)\n",
    "        output = tf.tanh(tf.matmul(inter1, self._kernel1) + self._bias1)\n",
    "        return output, output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN cell with 3 intermediate layers before output hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyBasicRNNCell_d4(tf.contrib.rnn.BasicRNNCell):\n",
    "\n",
    "    def build(self, inputs_shape):\n",
    "\n",
    "        input_depth = inputs_shape[1].value\n",
    "        \n",
    "        self._kernel = self.add_variable(name=\"kernel_hidden\", shape=[input_depth + self._num_units, self._num_units])\n",
    "        self._bias = self.add_variable(name=\"bias_hidden\", shape=[self._num_units], initializer=tf.zeros_initializer())\n",
    "        self._kernel1 = self.add_variable(name=\"kernel_hidden1\", shape=[self._num_units, self._num_units])\n",
    "        self._bias1 = self.add_variable(name=\"bias_hidden1\", shape=[self._num_units], initializer=tf.zeros_initializer())\n",
    "        self._kernel2 = self.add_variable(name=\"kernel_hidden2\", shape=[self._num_units, self._num_units])\n",
    "        self._bias2 = self.add_variable(name=\"bias_hidden2\", shape=[self._num_units], initializer=tf.zeros_initializer())\n",
    "        self._kernel3 = self.add_variable(name=\"kernel_hidden3\", shape=[self._num_units, self._num_units])\n",
    "        self._bias3 = self.add_variable(name=\"bias_hidden3\", shape=[self._num_units], initializer=tf.zeros_initializer())\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, inputs, state):\n",
    "        \"\"\"Most basic RNN: output = new_state = act(W * input + U * state + B).\"\"\"\n",
    "        \n",
    "        inter1 = tf.tanh(tf.matmul(tf.concat([inputs, state], 1), self._kernel) + self._bias)\n",
    "        inter2 = tf.tanh(tf.matmul(inter1, self._kernel1) + self._bias1)\n",
    "        inter3 = tf.tanh(tf.matmul(inter2, self._kernel2) + self._bias2)\n",
    "        output = tf.tanh(tf.matmul(inter3, self._kernel3) + self._bias3)\n",
    "\n",
    "\n",
    "        return output, output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic LSTM cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "LSTMStateTuple = collections.namedtuple(\"LSTMStateTuple\", (\"c\", \"h\"))\n",
    "\n",
    "class MyBasicLSTMCell(tf.contrib.rnn.BasicLSTMCell):\n",
    "\n",
    "    def build(self, inputs_shape):\n",
    "\n",
    "        input_depth = inputs_shape[1].value\n",
    "        self._kernel = self.add_variable(name=\"kernel\", shape=[input_depth + self._num_units, 4 * self._num_units])\n",
    "        self._bias = self.add_variable(name=\"bias\", shape=[4 * self._num_units], initializer=tf.zeros_initializer())\n",
    "\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, inputs, state):\n",
    "\n",
    "        one = tf.constant(1, dtype=tf.int32)\n",
    "        c, h = state\n",
    "\n",
    "        gate_inputs = tf.matmul(tf.concat([inputs, h], axis=1), self._kernel) + self._bias\n",
    "\n",
    "        input_gate_weights, input_weights, forget_gate_weights, output_gate_weights = tf.split(\n",
    "            value=gate_inputs, num_or_size_splits=4, axis=one)\n",
    "\n",
    "        # forget gating\n",
    "        forget_bias_tensor = tf.constant(1.0, dtype=forget_gate_weights.dtype)\n",
    "        forget_gate = tf.sigmoid(forget_gate_weights + forget_bias_tensor)\n",
    "        gated_memory = c * forget_gate\n",
    "\n",
    "        # input gating\n",
    "        processed_new_input = tf.tanh(input_weights)\n",
    "        input_gate = tf.sigmoid(input_gate_weights)\n",
    "        gated_input = input_gate * processed_new_input\n",
    "\n",
    "        # updating memory\n",
    "        new_c = gated_memory + gated_input\n",
    "\n",
    "        # output gating\n",
    "        processed_memory = tf.tanh(new_c)\n",
    "        output_gate = tf.sigmoid(output_gate_weights)\n",
    "        new_h = processed_memory * output_gate\n",
    "\n",
    "        new_state = tf.nn.rnn_cell.LSTMStateTuple(new_c, new_h)\n",
    "\n",
    "        return new_h, new_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic LSTM cell without gating of input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "LSTMStateTuple = collections.namedtuple(\"LSTMStateTuple\", (\"c\", \"h\"))\n",
    "\n",
    "class MyBasicLSTMCell_lessgate(tf.contrib.rnn.BasicLSTMCell):\n",
    "\n",
    "    def build(self, inputs_shape):\n",
    "\n",
    "        input_depth = inputs_shape[1].value\n",
    "        self._kernel = self.add_variable(name=\"kernel\", shape=[input_depth + self._num_units, 4 * self._num_units])\n",
    "        self._bias = self.add_variable(name=\"bias\", shape=[4 * self._num_units], initializer=tf.zeros_initializer())\n",
    "\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, inputs, state):\n",
    "\n",
    "        one = tf.constant(1, dtype=tf.int32)\n",
    "        c, h = state\n",
    "\n",
    "        gate_inputs = tf.matmul(tf.concat([inputs, h], axis=1), self._kernel) + self._bias\n",
    "\n",
    "        input_gate_weights, input_weights, forget_gate_weights, output_gate_weights = tf.split(\n",
    "            value=gate_inputs, num_or_size_splits=4, axis=one)\n",
    "\n",
    "        # forget gating\n",
    "        forget_bias_tensor = tf.constant(1.0, dtype=forget_gate_weights.dtype)\n",
    "        forget_gate = tf.sigmoid(forget_gate_weights + forget_bias_tensor)\n",
    "        gated_memory = c * forget_gate\n",
    "\n",
    "        # input gating\n",
    "        processed_new_input = tf.tanh(input_weights)\n",
    "        #input_gate = tf.sigmoid(input_gate_weights)\n",
    "        #gated_input = input_gate * processed_new_input\n",
    "\n",
    "        # updating memory\n",
    "        new_c = gated_memory + processed_new_input #gated_input\n",
    "\n",
    "        # output gating\n",
    "        processed_memory = tf.tanh(new_c)\n",
    "        output_gate = tf.sigmoid(output_gate_weights)\n",
    "        new_h = processed_memory * output_gate\n",
    "\n",
    "        new_state = tf.nn.rnn_cell.LSTMStateTuple(new_c, new_h)\n",
    "\n",
    "        return new_h, new_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create network, by unrolling cell to length inputs_list - length of inputs, and the cross entropy loss for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def network(myLSTMCell,inputs,targets):\n",
    "\n",
    "    with tf.variable_scope('embedding_matrix'):\n",
    "        embedding = tf.get_variable('embedding', [args.num_chars, args.num_units])\n",
    "        embedded_inputs = tf.nn.embedding_lookup(embedding, inputs)\n",
    "        inputs_list = tf.unstack(embedded_inputs, axis=1)  # shape: a list of [batch_size, num_units] length num_steps\n",
    "\n",
    "    with tf.variable_scope('LSTMCell') as myscope:\n",
    "        cell = myLSTMCell(args.num_units)\n",
    "        init_state = cell.zero_state(args.batch_size, tf.float32)\n",
    "        state = init_state\n",
    "        outputs = []\n",
    "\n",
    "        for time_, input in enumerate(inputs_list):\n",
    "            if time_ > 0:\n",
    "                myscope.reuse_variables()\n",
    "           \n",
    "            output, state = cell(input, state)\n",
    "            outputs.append(output)\n",
    "    # All hidden outputs for each batch and every step in the batch are reshaped\n",
    "    # as one long matrix to be transformed to logits and compared to targets.\n",
    "        output_reshaped = tf.reshape(tf.concat(outputs, 1), [-1, args.num_units])\n",
    "\n",
    "        final_state = state\n",
    "\n",
    "    with tf.variable_scope('regression'):\n",
    "        W = tf.get_variable('W', [args.num_units, args.num_chars])\n",
    "        b = tf.get_variable('b', [args.num_chars], initializer=tf.constant_initializer(0.0))\n",
    "        logits = tf.matmul(output_reshaped, W) + b\n",
    "        prob = tf.nn.softmax(logits)\n",
    "\n",
    "    with tf.variable_scope('cost'):\n",
    "        targets_straightened = tf.reshape(targets, [-1])\n",
    "        crossentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, \n",
    "                                                    labels=targets_straightened)\n",
    "        loss = tf.reduce_mean(crossentropy)\n",
    "        cost = loss/args.batch_size/args.num_steps\n",
    "\n",
    "    with tf.variable_scope('optimizer'):\n",
    "        train_step = tf.train.AdamOptimizer(args.lr).minimize(loss)\n",
    "\n",
    "    with tf.variable_scope('saver'):\n",
    "        saver = tf.train.Saver()\n",
    "    return init_state, train_step, loss, final_state, saver, prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainer(myCell,num_batches=None):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    tf.reset_default_graph()\n",
    "    # Define the placeholders\n",
    "    with tf.variable_scope('placeholders'):\n",
    "            inputs = tf.placeholder(tf.int32, [args.batch_size, args.num_steps])\n",
    "            targets = tf.placeholder(tf.int32, [args.batch_size, args.num_steps])\n",
    "    # Create the network\n",
    "    init_state, train_step, loss, final_state, saver, prob=network(myCell,inputs,targets)\n",
    "    print('train_num_batches',loader.train_num_batches)\n",
    "    \n",
    "    if (num_batches is None):\n",
    "        num_batches=loader.train_num_batches\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "         \n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        # computation graph for training\n",
    "        training_losses = []\n",
    "\n",
    "        for epoch in range(args.num_epochs):\n",
    "            loader.reset_batch_pointer()\n",
    "            state_ = sess.run(init_state)\n",
    "            training_loss = 0\n",
    "\n",
    "            for batch in range(num_batches):\n",
    "                #state_ = sess.run(init_state)\n",
    "                x, y = loader.next_batch_train()\n",
    "\n",
    "                feed_dict = dict()\n",
    "                feed_dict[inputs] = x\n",
    "                feed_dict[targets] = y\n",
    "                \n",
    "                if ('RNN' in myCell.__name__):\n",
    "                    feed_dict[init_state] = state_\n",
    "                else:\n",
    "                    feed_dict[init_state.c] = state_.c\n",
    "                    feed_dict[init_state.h] = state_.h\n",
    "\n",
    "                train_loss_, state_, _ = sess.run([loss, final_state, train_step], feed_dict=feed_dict)\n",
    "                training_loss += train_loss_\n",
    "            training_loss=training_loss/num_batches\n",
    "            print('epoch:', epoch, 'loss:',  training_loss)\n",
    "            training_losses.append(training_loss)\n",
    "        saver.save(sess, os.path.join(args.save_dir, 'saved_model'))\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    timer(start_time, end_time)\n",
    "    return(training_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run testing by getting the cross entropy loss on test batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Tester(MyCell, num_batches=None):\n",
    "    tf.reset_default_graph()\n",
    "    with tf.variable_scope('placeholders'):\n",
    "            inputs = tf.placeholder(tf.int32, [args.batch_size, args.num_steps])\n",
    "            targets = tf.placeholder(tf.int32, [args.batch_size, args.num_steps])\n",
    "    init_state, train_step, loss, final_state, saver, prob=network(MyCell,inputs,targets)\n",
    "\n",
    "    # Define initialization\n",
    "    \n",
    "    loader= TextLoader(args.data_dir, batch_size=args.batch_size, seq_length=args.num_steps)\n",
    "\n",
    " \n",
    "    if (num_batches is None):\n",
    "        num_batches=loader.test_num_batches\n",
    "    with tf.Session() as sess:\n",
    "\n",
    "        # Load saved model\n",
    "        saver.restore(sess, 'saved_model')\n",
    "        state_ = sess.run(init_state)\n",
    "\n",
    "        loader.reset_batch_pointer()\n",
    "        \n",
    "         # Get test error loss\n",
    "        test_loss = 0\n",
    "        print('num_batches',num_batches)\n",
    "\n",
    "        for batch in range(num_batches):\n",
    "\n",
    "            x, y = loader.next_batch_test()\n",
    "\n",
    "            feed_dict = dict()\n",
    "            feed_dict[inputs] = x\n",
    "            feed_dict[targets] = y\n",
    "\n",
    "            if ('RNN' in MyCell.__name__):\n",
    "                feed_dict[init_state] = state_\n",
    "            else:\n",
    "                feed_dict[init_state.c] = state_.c\n",
    "                feed_dict[init_state.h] = state_.h\n",
    "\n",
    "            test_loss_, state_= sess.run([loss, final_state], feed_dict=feed_dict)\n",
    "            test_loss += test_loss_\n",
    "        test_loss=test_loss/num_batches\n",
    "        print('test loss:',  test_loss)\n",
    "\n",
    "\n",
    "    \n",
    "    return(test_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synthesize text using an initializing text running the network on the initializing text and then contiunuing with the final hidden state to simulate new text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Synthesize(MyCell,init_string=\"None\"):\n",
    "    tf.reset_default_graph()\n",
    "    num_steps_bak=args.num_steps\n",
    "    batch_size_bak=args.batch_size\n",
    "    args.num_steps=1\n",
    "    args.batch_size=1\n",
    "    with tf.variable_scope('placeholders'):\n",
    "            inputs = tf.placeholder(tf.int32, [args.batch_size, args.num_steps])\n",
    "            targets = tf.placeholder(tf.int32, [args.batch_size, args.num_steps])\n",
    "    init_state, train_step, loss, final_state, saver, prob=network(MyCell,inputs,targets)\n",
    "\n",
    "    # Define initialization\n",
    "    if (init_string is None):\n",
    "        initialization = 'Where are you going today?'\n",
    "    else:\n",
    "        initialization = init_string\n",
    "    loader= TextLoader(args.data_dir, batch_size=1, seq_length=1)\n",
    "\n",
    "    forecast_data=np.array(list(map(loader.vocab_to_idx.get, initialization)))\n",
    "    print(forecast_data)\n",
    "    forecast_range = 100\n",
    "    top_k=5\n",
    " \n",
    "    with tf.Session() as sess:\n",
    "\n",
    "        # Load saved model\n",
    "        saver.restore(sess, 'saved_model')\n",
    "        state_ = sess.run(init_state)\n",
    "\n",
    "        # Run rnn on initialization data to get final hidden state before simulation\n",
    "        state_ = sess.run(init_state)\n",
    "        for i in range(forecast_data.shape[0]):\n",
    "\n",
    "            feed_dict = dict()\n",
    "            # Feed current predicted\n",
    "            feed_dict[inputs] = forecast_data[i].reshape(args.batch_size, args.num_steps)\n",
    "            if ('RNN' in MyCell.__name__):\n",
    "                feed_dict[init_state] = state_\n",
    "            else:\n",
    "                feed_dict[init_state.c] = state_.c\n",
    "                feed_dict[init_state.h] = state_.h\n",
    "            # Get new hidden state and prediction probabilities\n",
    "            predicted_prob, state_ = sess.run([prob, final_state], feed_dict=feed_dict)\n",
    "\n",
    "        # last state of this step becomes first state of simulation\n",
    "\n",
    "        for i in range(forecast_range):\n",
    "\n",
    "            feed_dict = dict()\n",
    "            # Feed current predicted\n",
    "            feed_dict[inputs] = forecast_data[-args.num_steps:].reshape(args.batch_size, args.num_steps)\n",
    "            if ('RNN' in MyCell.__name__):\n",
    "                feed_dict[init_state] = state_\n",
    "            else:\n",
    "                feed_dict[init_state.c] = state_.c\n",
    "                feed_dict[init_state.h] = state_.h\n",
    "            predicted_prob, state_ = sess.run([prob, final_state], feed_dict=feed_dict)\n",
    "\n",
    "            predicted_prob = predicted_prob.ravel()\n",
    "            # Simulate from top top_k probs\n",
    "            predicted_prob[np.argsort(predicted_prob)[:-top_k]] = 0\n",
    "            predicted_prob = predicted_prob/np.sum(predicted_prob)\n",
    "            sample = np.random.choice(args.num_chars, 1, p=predicted_prob)[0]\n",
    "\n",
    "\n",
    "            forecast_data = np.hstack((forecast_data, sample))\n",
    "\n",
    "    forecasted_chars = np.asarray([loader.idx_to_vocab[elem] for elem in forecast_data])\n",
    "\n",
    "    print(''.join(forecasted_chars))\n",
    "  \n",
    "    args.num_steps=num_steps_bak\n",
    "    args.batch_size=batch_size_bak"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running the different models and testing them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_num_batches 185\n",
      "epoch: 0 loss: 2.375254158715944\n",
      "epoch: 1 loss: 1.8126878319559871\n",
      "epoch: 2 loss: 1.6422542526915267\n",
      "epoch: 3 loss: 1.548723560410577\n",
      "epoch: 4 loss: 1.4858037394446295\n",
      "epoch: 5 loss: 1.4417319639309032\n",
      "epoch: 6 loss: 1.408230753202696\n",
      "epoch: 7 loss: 1.3803289381233421\n",
      "epoch: 8 loss: 1.3581474445961617\n",
      "epoch: 9 loss: 1.3408515459782369\n",
      "epoch: 10 loss: 1.3245993962159028\n",
      "epoch: 11 loss: 1.3103126751409995\n",
      "epoch: 12 loss: 1.2975761716430252\n",
      "epoch: 13 loss: 1.2865822212116138\n",
      "epoch: 14 loss: 1.2747626349732684\n",
      "epoch: 15 loss: 1.264664246584918\n",
      "epoch: 16 loss: 1.2557935772715387\n",
      "epoch: 17 loss: 1.2484358072280883\n",
      "epoch: 18 loss: 1.2407760864979511\n",
      "epoch: 19 loss: 1.2327337368114575\n",
      "epoch: 20 loss: 1.225172453957635\n",
      "epoch: 21 loss: 1.2180879947301504\n",
      "epoch: 22 loss: 1.211738293879741\n",
      "epoch: 23 loss: 1.2060794798103538\n",
      "epoch: 24 loss: 1.2014588124043233\n",
      "epoch: 25 loss: 1.1996361242758262\n",
      "epoch: 26 loss: 1.1971267184695682\n",
      "epoch: 27 loss: 1.1944619088559538\n",
      "epoch: 28 loss: 1.1913632792395514\n",
      "epoch: 29 loss: 1.1870138980246878\n",
      "00 hours 04 minutes 36.73 seconds\n"
     ]
    }
   ],
   "source": [
    "re_lstm_basic = trainer(MyBasicLSTMCell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are preprocessed data - lets load it\n",
      "[39  5  1  7  1  0  4  7  1  0 15  3 13  0 20  3  9  8 20  0  2  3 12  4\n",
      " 15 44]\n",
      "INFO:tensorflow:Restoring parameters from saved_model\n",
      "num_batches 47\n",
      "test loss: 1.4047210089703823\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.4047210089703823"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Tester(MyBasicLSTMCell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are preprocessed data - lets load it\n",
      "[39  5  4  2  0  9  6  0  2  5  1  0 14  1  4  8  9  8 20  0  3 18  0 11\n",
      "  9 18  1]\n",
      "INFO:tensorflow:Restoring parameters from saved_model\n",
      "What is the meaning of lifen.\n",
      "\n",
      "DUCHESS OF YORK:\n",
      "Well, good father, this is a present days,\n",
      "The play'd thy heatter, when you hav\n"
     ]
    }
   ],
   "source": [
    "Synthesize(MyBasicLSTMCell,\"What is the meaning of life\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_num_batches 185\n",
      "epoch: 0 loss: 2.4314659956339244\n",
      "epoch: 1 loss: 1.8905949940552582\n",
      "epoch: 2 loss: 1.7162846378377965\n",
      "epoch: 3 loss: 1.615376700581731\n",
      "epoch: 4 loss: 1.5499827417167458\n",
      "epoch: 5 loss: 1.501998496055603\n",
      "epoch: 6 loss: 1.4657965608545251\n",
      "epoch: 7 loss: 1.436786049121135\n",
      "epoch: 8 loss: 1.4134604975983904\n",
      "epoch: 9 loss: 1.3927866967948708\n",
      "epoch: 10 loss: 1.3778861980180483\n",
      "epoch: 11 loss: 1.3615304160762478\n",
      "epoch: 12 loss: 1.3485511876441336\n",
      "epoch: 13 loss: 1.336150902670783\n",
      "epoch: 14 loss: 1.3274670323810063\n",
      "epoch: 15 loss: 1.318680637591594\n",
      "epoch: 16 loss: 1.3076662437335864\n",
      "epoch: 17 loss: 1.299986473289696\n",
      "epoch: 18 loss: 1.293395793760145\n",
      "epoch: 19 loss: 1.2851229042620271\n",
      "epoch: 20 loss: 1.2790844272922826\n",
      "epoch: 21 loss: 1.275701347557274\n",
      "epoch: 22 loss: 1.267823540842211\n",
      "epoch: 23 loss: 1.2612911707646137\n",
      "epoch: 24 loss: 1.2561672874399135\n",
      "epoch: 25 loss: 1.2542885973646833\n",
      "epoch: 26 loss: 1.2479632893124142\n",
      "epoch: 27 loss: 1.2416488106186325\n",
      "epoch: 28 loss: 1.237960881800265\n",
      "epoch: 29 loss: 1.2323213667482944\n",
      "00 hours 04 minutes 09.79 seconds\n"
     ]
    }
   ],
   "source": [
    "re_lstm_lessgate=trainer(MyBasicLSTMCell_lessgate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are preprocessed data - lets load it\n",
      "INFO:tensorflow:Restoring parameters from saved_model\n",
      "num_batches 47\n",
      "test loss: 1.3992189143566376\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.3992189143566376"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Tester(MyBasicLSTMCell_lessgate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_num_batches 185\n",
      "epoch: 0 loss: 2.2866439722679757\n",
      "epoch: 1 loss: 1.8399305562715274\n",
      "epoch: 2 loss: 1.6943824864722588\n",
      "epoch: 3 loss: 1.6129409409858084\n",
      "epoch: 4 loss: 1.5597342678018518\n",
      "epoch: 5 loss: 1.5227948936256201\n",
      "epoch: 6 loss: 1.495543941291603\n",
      "epoch: 7 loss: 1.4738708856943492\n",
      "epoch: 8 loss: 1.4562455061319712\n",
      "epoch: 9 loss: 1.4417609330770131\n",
      "epoch: 10 loss: 1.4296678446434639\n",
      "epoch: 11 loss: 1.4196067913158519\n",
      "epoch: 12 loss: 1.4109132090130367\n",
      "epoch: 13 loss: 1.4029981858021505\n",
      "epoch: 14 loss: 1.3962290828292434\n",
      "epoch: 15 loss: 1.390509927594984\n",
      "epoch: 16 loss: 1.3851184748314522\n",
      "epoch: 17 loss: 1.3802465155317976\n",
      "epoch: 18 loss: 1.375983655130541\n",
      "epoch: 19 loss: 1.3717353930344454\n",
      "epoch: 20 loss: 1.3677776613750974\n",
      "epoch: 21 loss: 1.3644523942792737\n",
      "epoch: 22 loss: 1.3617173523516268\n",
      "epoch: 23 loss: 1.3598538508286346\n",
      "epoch: 24 loss: 1.3585559806308232\n",
      "epoch: 25 loss: 1.3561777404836706\n",
      "epoch: 26 loss: 1.3540037986394522\n",
      "epoch: 27 loss: 1.3522032595969535\n",
      "epoch: 28 loss: 1.3499840027577168\n",
      "epoch: 29 loss: 1.3480139964335673\n",
      "00 hours 02 minutes 04.07 seconds\n"
     ]
    }
   ],
   "source": [
    "re_rnn_basic = trainer(MyBasicRNNCell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are preprocessed data - lets load it\n",
      "INFO:tensorflow:Restoring parameters from saved_model\n",
      "num_batches 47\n",
      "test loss: 1.4636335271470091\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.4636335271470091"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Tester(MyBasicRNNCell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are preprocessed data - lets load it\n",
      "[39  5  4  2  0  9  6  0  2  5  1  0 14  1  4  8  9  8 20  0  3 18  0 11\n",
      "  9 18  1]\n",
      "INFO:tensorflow:Restoring parameters from saved_model\n",
      "What is the meaning of lifer\n",
      "And told to the bride the king of command thee\n",
      "As thou sell mine earth.\n",
      "Where in her son,\n",
      "And man \n"
     ]
    }
   ],
   "source": [
    "Synthesize(MyBasicRNNCell,\"What is the meaning of life\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_num_batches 185\n",
      "epoch: 0 loss: 2.271846804103336\n",
      "epoch: 1 loss: 1.8089590768556336\n",
      "epoch: 2 loss: 1.6572276076755008\n",
      "epoch: 3 loss: 1.5718503616951607\n",
      "epoch: 4 loss: 1.5170617232451569\n",
      "epoch: 5 loss: 1.4790298552126497\n",
      "epoch: 6 loss: 1.450774130950103\n",
      "epoch: 7 loss: 1.4293345013180294\n",
      "epoch: 8 loss: 1.412054891199679\n",
      "epoch: 9 loss: 1.3972392713701403\n",
      "epoch: 10 loss: 1.3849842425939198\n",
      "epoch: 11 loss: 1.3753085535925789\n",
      "epoch: 12 loss: 1.3667957331683185\n",
      "epoch: 13 loss: 1.3601067504367312\n",
      "epoch: 14 loss: 1.3553066330987054\n",
      "epoch: 15 loss: 1.3503588805327544\n",
      "epoch: 16 loss: 1.3472831526318112\n",
      "epoch: 17 loss: 1.3440541776450905\n",
      "epoch: 18 loss: 1.3393264255008182\n",
      "epoch: 19 loss: 1.3336764258307379\n",
      "epoch: 20 loss: 1.3311266744458998\n",
      "epoch: 21 loss: 1.3299312829971313\n",
      "epoch: 22 loss: 1.3276082122648085\n",
      "epoch: 23 loss: 1.323850263776006\n",
      "epoch: 24 loss: 1.3204784064679533\n",
      "epoch: 25 loss: 1.3190409705445574\n",
      "epoch: 26 loss: 1.3163784536155494\n",
      "epoch: 27 loss: 1.3159131681596912\n",
      "epoch: 28 loss: 1.31591013508874\n",
      "epoch: 29 loss: 1.3148499353511913\n",
      "00 hours 02 minutes 47.22 seconds\n"
     ]
    }
   ],
   "source": [
    "re_rnn_basic_d2 = trainer(MyBasicRNNCell_d2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are preprocessed data - lets load it\n",
      "[39  5  1  7  1  0  4  7  1  0 15  3 13  0 20  3  9  8 20  0  2  3 12  4\n",
      " 15 44]\n",
      "INFO:tensorflow:Restoring parameters from saved_model\n",
      "num_batches 47\n",
      "test loss: 1.4272671354577897\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.4272671354577897"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Tester(MyBasicRNNCell_d2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_num_batches 185\n",
      "epoch: 0 loss: 2.408355314022786\n",
      "epoch: 1 loss: 1.8452126780071774\n",
      "epoch: 2 loss: 1.6803465772319484\n",
      "epoch: 3 loss: 1.5881870450200262\n",
      "epoch: 4 loss: 1.528356367188531\n",
      "epoch: 5 loss: 1.486127984201586\n",
      "epoch: 6 loss: 1.4557582365499961\n",
      "epoch: 7 loss: 1.4338586981232102\n",
      "epoch: 8 loss: 1.416147020056441\n",
      "epoch: 9 loss: 1.404949699865805\n",
      "epoch: 10 loss: 1.3931776439821397\n",
      "epoch: 11 loss: 1.380185132413297\n",
      "epoch: 12 loss: 1.3715257258028597\n",
      "epoch: 13 loss: 1.3649000328940315\n",
      "epoch: 14 loss: 1.3596686053920437\n",
      "epoch: 15 loss: 1.3541683390333845\n",
      "epoch: 16 loss: 1.3500864331786697\n",
      "epoch: 17 loss: 1.3480135717907467\n",
      "epoch: 18 loss: 1.34493774401175\n",
      "epoch: 19 loss: 1.341222183124439\n",
      "epoch: 20 loss: 1.3374746870350194\n",
      "epoch: 21 loss: 1.3403148825104172\n",
      "epoch: 22 loss: 1.3379634831402754\n",
      "epoch: 23 loss: 1.3488434411383965\n",
      "epoch: 24 loss: 1.3350426081064586\n",
      "epoch: 25 loss: 1.3343019118180146\n",
      "epoch: 26 loss: 1.3386835620209978\n",
      "epoch: 27 loss: 1.3370027116827063\n",
      "epoch: 28 loss: 1.3370941954690057\n",
      "epoch: 29 loss: 1.348004902375711\n",
      "00 hours 04 minutes 42.71 seconds\n"
     ]
    }
   ],
   "source": [
    "re_rnn_basic_d4 = trainer(MyBasicRNNCell_d4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are preprocessed data - lets load it\n",
      "INFO:tensorflow:Restoring parameters from saved_model\n",
      "num_batches 47\n",
      "test loss: 1.4106874440578705\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.4106874440578705"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Tester(MyBasicRNNCell_d4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
