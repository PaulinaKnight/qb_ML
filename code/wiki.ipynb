{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/amit/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import string\n",
    "import itertools\n",
    "import time\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "713069767\n",
      "1601\n"
     ]
    }
   ],
   "source": [
    "def load_wiki_data():\n",
    "    wiki_file = open('/Users/amit/Box Sync/lsda-2018/wikipedia/wiki-text.txt', 'r').readlines()[0]\n",
    "    print(len(wiki_file))\n",
    "    num_chunks = 1600\n",
    "    chunks, chunk_size = len(wiki_file), np.int32(len(wiki_file)/num_chunks)\n",
    "    wiki_chunks = [wiki_file[i:i+chunk_size] for i in np.int32(np.arange(0, chunks, chunk_size))]\n",
    "    return wiki_chunks\n",
    "    \n",
    "    \n",
    "wiki_data = load_wiki_data()\n",
    "print(len(wiki_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81535178\n",
      "Time to split 20.645808219909668\n"
     ]
    }
   ],
   "source": [
    "t1=time.time()\n",
    "all_words=[]\n",
    "\n",
    "for w in wiki_data:\n",
    "    all_words.append(w.split())\n",
    "aw=[w for a in all_words for w in a if not w in stop_words]\n",
    "print(len(aw))\n",
    "print(\"Time to split\",time.time()-t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81535178\n",
      "Time to create vocab and clean words 102.00454592704773\n",
      "Number of clean words 71618326\n",
      "Vocab size 13201\n"
     ]
    }
   ],
   "source": [
    "t1=time.time()\n",
    "all_words=aw\n",
    "s_all_words = sorted(all_words)\n",
    "print(len(s_all_words))\n",
    "gg=itertools.groupby(s_all_words)\n",
    "word_counts=[(k,len(list(g))) for k,g in gg]\n",
    "\n",
    "dvocab={}\n",
    "t=0\n",
    "for i,x in enumerate(word_counts):\n",
    "    if (x[1]>500):\n",
    "        dvocab[x[0]]=t\n",
    "        t+=1\n",
    "\n",
    "clean_words=[]\n",
    "for w in all_words:\n",
    "    if w in dvocab:\n",
    "        clean_words.append((w,dvocab[w]))\n",
    "print(\"Time to create vocab and clean words\",time.time()-t1)\n",
    "print('Number of clean words',len(clean_words))\n",
    "print('Vocab size',len(list(dvocab.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cooccurence(words, wnd_size):\n",
    "    rv = {}\n",
    "    for kk in np.arange(1, len(words),1):\n",
    "        w_ii = words[kk][0]\n",
    "        ind_ii=words[kk][1]\n",
    "        left = max(0, kk - wnd_size)\n",
    "        for jj in np.arange(left,kk,1):\n",
    "            w_jj=words[jj][0]\n",
    "            ind_jj=words[jj][1]\n",
    "            if (w_ii, w_jj) not in rv:\n",
    "                rv[(w_ii, w_jj)] = [1.,ind_ii,ind_jj]\n",
    "            else:\n",
    "                rv[(w_ii, w_jj)][0] += 1.\n",
    "    return(rv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cooccurence_full(words, wnd_size):\n",
    "    MM=np.zeros((vocab_size,vocab_size),dtype=np.int32)\n",
    "    for kk in np.arange(1, len(words),1):\n",
    "        w_ii = words[kk][0]\n",
    "        ind_ii=words[kk][1]\n",
    "        left = max(0, kk - wnd_size)\n",
    "        for jj in np.arange(left,kk,1):\n",
    "            w_jj=words[jj][0]\n",
    "            ind_jj=words[jj][1]\n",
    "            MM[ind_ii,ind_jj]+=1\n",
    "           \n",
    "    return(MM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len clean_words 71618326\n",
      "13201\n",
      "Time to compute matrix 832.2892348766327\n",
      "Time to compute matrix and svd 82.22771096229553\n"
     ]
    }
   ],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "from scipy.sparse.linalg import svds\n",
    "import numpy as np\n",
    "\n",
    "print('len clean_words',len(clean_words))\n",
    "vocab_size = len(list(dvocab.keys()))\n",
    "print(vocab_size)\n",
    "\n",
    "t1=time.time()\n",
    "D=cooccurence_full(clean_words,5)\n",
    "print(\"Time to compute matrix\",time.time()-t1)\n",
    "\n",
    "D=np.float32(D)\n",
    "t1=time.time()\n",
    "M = D + D.T\n",
    "word_cnt_vec = D.sum(axis=1).T\n",
    "M = np.count_nonzero(M) * (M + 1)\n",
    "M = np.log(M / word_cnt_vec.dot(word_cnt_vec.T))\n",
    "U,s,V = svds(csr_matrix(M), k=50)\n",
    "W = U.dot(np.diag(np.sqrt(s)))\n",
    "print(\"Time to compute matrix and svd\",time.time()-t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1969 1911\n",
      "nine\n"
     ]
    }
   ],
   "source": [
    "i=np.mod(np.argmax(D),3078)\n",
    "j=np.argmax(D)//3078\n",
    "print(i,j)\n",
    "print(list(dvocab.keys())[1911])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_vec={}\n",
    "\n",
    "for i,w in zip(dvocab.values(),dvocab.keys()):\n",
    "    word_to_vec[w]=W[i,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_closest_word(word,K):\n",
    "    words = list(word_to_vec.keys())\n",
    "    target = word_to_vec[word]\n",
    "    dist = np.array([np.linalg.norm(target-word_to_vec[el]) for el in words])\n",
    "    ind = np.argsort(dist)[1:(K+1)]\n",
    "    print(ind)\n",
    "    rv = [words[ii] for ii in ind]\n",
    "    return rv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_closest_word_to_v(v,K):\n",
    "    words = list(word_to_vec.keys())\n",
    "    target = v\n",
    "    dist = np.array([np.linalg.norm(target-word_to_vec[el]) for el in words])\n",
    "    ind = np.argsort(dist)[1:(K+1)]\n",
    "    print(ind)\n",
    "    rv = [words[ii] for ii in ind]\n",
    "    return rv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7877 5726 8786  507 6572]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['nature', 'humans', 'physical', 'animal', 'knowledge']"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v=word_to_vec['human']-word_to_vec['baby']+word_to_vec['cow']\n",
    "find_closest_word_to_v(v,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
